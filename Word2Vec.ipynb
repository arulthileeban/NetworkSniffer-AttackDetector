{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, '+': 11, ',': 12, '-': 13, '.': 14, '/': 15, '0': 16, '1': 17, '2': 18, '3': 19, '4': 20, '5': 21, '6': 22, '7': 23, '8': 24, '9': 25, ':': 26, ';': 27, '<': 28, '=': 29, '>': 30, '?': 31, '@': 32, '[': 33, '\\\\': 34, ']': 35, '^': 36, '_': 37, '`': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64, '{': 65, '|': 66, '}': 67, '~': 68}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re,string\n",
    "sentences = []\n",
    "with open('normalTrafficTraining.txt') as f:\n",
    "    arr = f.readlines()\n",
    "    content = ''.join(arr)\n",
    "    items=re.findall(\"^GET.*|POST.*$\",content,re.MULTILINE)\n",
    "    data=[]\n",
    "    data_string=\"\"\n",
    "    for item in items:\n",
    "        temp_data=item.replace(\" HTTP/1.1\",\"\").lower()\n",
    "        data.append(temp_data)\n",
    "        #data_string+=temp_data\n",
    "    chars = sorted((string.ascii_lowercase+string.punctuation+string.digits+\" \"))\n",
    "    n_vocab = len(chars)\n",
    "    data_map = dict((c, i) for i, c in enumerate(chars))\n",
    "    print(data_map)\n",
    "    \n",
    "#model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  36000\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "dataX = []\n",
    "dataY = []\n",
    "max_val=0\n",
    "for data_val in data:\n",
    "    if len(data_val)>max_val:\n",
    "        max_val = len(data_val)\n",
    "seq_length = max_val\n",
    "for data_val in data:\n",
    "    seq_in = data_val[:len(data_val)-1]\n",
    "    seq_out = data_val[1:]\n",
    "    temp_data_in = [-1]*(max_val-len(seq_in))\n",
    "    \n",
    "    dataX.append([data_map[data_char] for data_char in seq_in]+ temp_data_in)\n",
    "    dataY.append([data_map[data_char] for data_char in seq_out])\n",
    "n_patterns = len(dataX)\n",
    "\n",
    "print(\"Total Patterns: \", n_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 362)\n",
      "362 36000\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.utils import np_utils\n",
    "temp = numpy.asarray(dataX)\n",
    "print(temp.shape)\n",
    "print(seq_length,len(dataX))\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y=[]\n",
    "for val in dataY:\n",
    "    y.append(np_utils.to_categorical(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
